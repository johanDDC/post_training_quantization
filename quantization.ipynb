{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-07-10T06:33:44.642993Z",
     "end_time": "2023-07-10T06:33:46.560690Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "from src.models.resnet20 import ResNet20\n",
    "from torch.utils.data import DataLoader\n",
    "from data.data import get_train_data\n",
    "from main import evaluate\n",
    "from src.utils import Timer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\r\n",
      "Files already downloaded and verified\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjohan_ddc\u001B[0m (\u001B[33mjohan_ddc_team\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.15.5 is available!  To upgrade, please run:\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.15.0\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/home/johan/PycharmProjects/quantization/wandb/run-20230708_201400-18g01tsv\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mresnet20_train\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/johan_ddc_team/quatization_simple\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/johan_ddc_team/quatization_simple/runs/18g01tsv\u001B[0m\r\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [16:40<00:00, 20.02s/it]\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish... \u001B[32m(success).\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run history:\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   accuracy ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         lr ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   val_loss ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run summary:\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   accuracy 0.90892\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:         lr 0.0\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: train_loss 1.11768\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   val_loss 0.38892\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run \u001B[33mresnet20_train\u001B[0m at: \u001B[34m\u001B[4mhttps://wandb.ai/johan_ddc_team/quatization_simple/runs/18g01tsv\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Find logs at: \u001B[35m\u001B[1m./wandb/run-20230708_201400-18g01tsv/logs\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-08T20:13:55.291913Z",
     "end_time": "2023-07-08T20:30:53.184155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def memory_consumption(model, bits):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    memory = num_params * bits / (8 * 1024 ** 2)\n",
    "    return memory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T06:33:46.563904Z",
     "end_time": "2023-07-10T06:33:46.566398Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Unquantized model on cpu took 48.01813s.\n",
      "\n",
      "Final test loss: 0.38893190026283264\n",
      "Final test accuracy: 0.9089201092720032\n",
      "(Theoretical) memory consumption of model: 1.117Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"Unquantized model on cpu\")\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "cifar10_train, cifar10_test = get_train_data(root_dir=\"data\")\n",
    "train_loader = DataLoader(cifar10_train, batch_size=128, shuffle=True, pin_memory=True, num_workers=1, drop_last=True)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=128, shuffle=False, pin_memory=True, num_workers=1)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model, criterion, test_loader)\n",
    "print()\n",
    "print(f\"Final test loss: {val_loss.item()}\")\n",
    "print(f\"Final test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 32), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T06:33:46.570380Z",
     "end_time": "2023-07-10T06:34:37.361521Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch Post Training Quantization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 16 bits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-bit quantized model took 3.99165s.\n",
      "Quantized model test loss: 0.38875532150268555\n",
      "Quantized model test accuracy: 0.9090189933776855\n",
      "(Theoretical) memory consumption of model: 0.559Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"16-bit quantized model\")\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.half()\n",
    "model.cuda()\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model, criterion, test_loader, device=\"cuda\",\n",
    "                                      batch_preprocessor=lambda batch: batch.half())\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 16), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:09:57.782544Z",
     "end_time": "2023-07-10T01:10:01.846459Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 8 bits with per tensor quantization scheme."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per tensor scheme) took 33.56244s.\n",
      "Quantized model test loss: 0.3915562331676483\n",
      "Quantized model test accuracy: 0.9067444801330566\n",
      "(Theoretical) memory consumption of model: 0.279Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"8-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(\n",
    "    activation=torch.ao.quantization.observer.MinMaxObserver.with_args(dtype=torch.quint8),\n",
    "    weight=torch.ao.quantization.observer.MinMaxObserver.with_args(dtype=torch.qint8,\n",
    "                                                                   qscheme=torch.per_tensor_symmetric)\n",
    ")\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 8), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:10:10.704463Z",
     "end_time": "2023-07-10T01:11:01.203360Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Post Training Quantization\n",
    "## Simple implementation using torch Quantization API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 8 bits (per tensor quantization scheme)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per tensor scheme) took 34.17681s.\n",
      "Quantized model test loss: 0.3917810916900635\n",
      "Quantized model test accuracy: 0.905656635761261\n",
      "(Theoretical) memory consumption of model: 0.279Mb\n"
     ]
    }
   ],
   "source": [
    "from src.ptq.activation_observer import SimpleObserver\n",
    "\n",
    "timer = Timer(\"8-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(SimpleObserver.with_args(dtype=torch.quint8),\n",
    "                                                      SimpleObserver.with_args(dtype=torch.qint8,\n",
    "                                                                               qscheme=torch.per_tensor_symmetric))\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 8), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:12:02.923052Z",
     "end_time": "2023-07-10T01:12:52.919568Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 4 bits (per tensor quantization scheme)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantized model (per tensor scheme) took 32.71681s.\n",
      "Quantized model test loss: 1.8605022430419922\n",
      "Quantized model test accuracy: 0.3407832384109497\n",
      "(Theoretical) memory consumption of model: 0.139Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"4-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(\n",
    "    SimpleObserver.with_args(dtype=torch.quint8, quant_min=0, quant_max=2 ** 4 - 1),\n",
    "    SimpleObserver.with_args(dtype=torch.qint8,\n",
    "                             qscheme=torch.per_tensor_symmetric, quant_min=-2 ** 3, quant_max=2 ** 3 - 1))\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 4), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T06:37:31.770647Z",
     "end_time": "2023-07-10T06:38:20.213761Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Model quantized to 2 bits (per tensor quantization scheme)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-bit quantized model (per tensor scheme) took 32.92772s.\n",
      "Quantized model test loss: 2.3025832176208496\n",
      "Quantized model test accuracy: 0.1002769023180008\n",
      "(Theoretical) memory consumption of model: 0.07Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"2-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(\n",
    "    SimpleObserver.with_args(dtype=torch.quint8, quant_min=0, quant_max=2 ** 2 - 1),\n",
    "    SimpleObserver.with_args(dtype=torch.qint8,\n",
    "                             qscheme=torch.per_tensor_symmetric, quant_min=-2, quant_max=1))\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 2), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T06:39:38.101933Z",
     "end_time": "2023-07-10T06:40:26.755066Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom quantization engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-bit quantized model (per tensor scheme) took 361.7651s.\n",
      "Quantized model test loss: 4.250782012939453\n",
      "Quantized model test accuracy: 0.09889240562915802\n",
      "(Theoretical) memory consumption of model: 0.557Mb\n"
     ]
    }
   ],
   "source": [
    "from src.ptq.model_quantizer import ModelQuantizer\n",
    "from src.ptq.activation_observer import SimpleObserver\n",
    "\n",
    "timer = Timer(\"16-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "mq = ModelQuantizer(model, SimpleObserver, num_bits=16, dtype=torch.int16)\n",
    "mq.calibrate()\n",
    "_, _ = evaluate(mq, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "mq.quantize()\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(mq, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 16), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:30:22.434040Z",
     "end_time": "2023-07-10T01:36:37.931168Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per tensor scheme) took 362.56452s.\n",
      "Quantized model test loss: 877.6320190429688\n",
      "Quantized model test accuracy: 0.09889240562915802\n",
      "(Theoretical) memory consumption of model: 0.279Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"8-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "mq = ModelQuantizer(model, SimpleObserver, num_bits=16, dtype=torch.int16, quant_min=0, quant_max=2 ** 8 - 1)\n",
    "mq.calibrate()\n",
    "_, _ = evaluate(mq, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "mq.quantize()\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(mq, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 8), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:36:37.931016Z",
     "end_time": "2023-07-10T01:42:54.828687Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantized model (per tensor scheme) took 353.24939s.\n",
      "Quantized model test loss: 15700.380859375\n",
      "Quantized model test accuracy: 0.09889240562915802\n",
      "(Theoretical) memory consumption of model: 0.139Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"4-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "mq = ModelQuantizer(model, SimpleObserver, num_bits=16, dtype=torch.int16, quant_min=0, quant_max=2 ** 4 - 1)\n",
    "mq.calibrate()\n",
    "_, _ = evaluate(mq, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "mq.quantize()\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(mq, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 4), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:42:54.834194Z",
     "end_time": "2023-07-10T01:49:01.738201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-bit quantized model (per tensor scheme) took 364.39447s.\n",
      "Quantized model test loss: 76492.9140625\n",
      "Quantized model test accuracy: 0.09889240562915802\n",
      "(Theoretical) memory consumption of model: 0.07Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"2-bit quantized model (per tensor scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "mq = ModelQuantizer(model, SimpleObserver, num_bits=16, dtype=torch.int16, quant_min=0, quant_max=2 ** 2 - 1)\n",
    "mq.calibrate()\n",
    "_, _ = evaluate(mq, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "mq.quantize()\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(mq, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 2), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T01:49:01.744936Z",
     "end_time": "2023-07-10T01:55:20.023365Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Additional experiments\n",
    "## Per channel quantization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 8-bit with per channel quantization scheme (native torch implementation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per channel scheme) took 34.25016s.\n",
      "Quantized model test loss: 0.383556067943573\n",
      "Quantized model test accuracy: 0.9060522317886353\n",
      "(Theoretical) memory consumption of model: 0.279Mb\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"8-bit quantized model (per channel scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 8), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T06:42:59.016026Z",
     "end_time": "2023-07-10T06:44:03.247195Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 8-bit with per channel quantization scheme (custom Observer implementation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per tensor scheme) took 32.75303s.\n",
      "Quantized model test loss: 0.4082055389881134\n",
      "Quantized model test accuracy: 0.9034810066223145\n",
      "(Theoretical) memory consumption of model: 0.279Mb\n"
     ]
    }
   ],
   "source": [
    "from src.ptq.activation_observer import PerChannelObserver, SimpleObserver\n",
    "\n",
    "timer = Timer(\"8-bit quantized model (per channel scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(SimpleObserver.with_args(dtype=torch.quint8,\n",
    "                                                                               qscheme=torch.per_tensor_affine),\n",
    "                                                      PerChannelObserver.with_args(dtype=torch.qint8,\n",
    "                                                                                   qscheme=torch.per_channel_symmetric))\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 8), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T07:21:13.241390Z",
     "end_time": "2023-07-10T07:22:02.305063Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 4-bit with per channel quantization scheme (custom Observer implementation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per tensor scheme) took 32.97311s.\n",
      "Quantized model test loss: 1.8132280111312866\n",
      "Quantized model test accuracy: 0.3720332384109497\n",
      "(Theoretical) memory consumption of model: 0.139Mb\n"
     ]
    }
   ],
   "source": [
    "from src.ptq.activation_observer import PerChannelObserver, SimpleObserver\n",
    "\n",
    "timer = Timer(\"4-bit quantized model (per channel scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(SimpleObserver.with_args(dtype=torch.quint8,\n",
    "                                                                               qscheme=torch.per_tensor_affine, quant_min=0, quant_max=2**4-1),\n",
    "                                                      PerChannelObserver.with_args(dtype=torch.qint8,\n",
    "                                                                                   qscheme=torch.per_channel_symmetric, quant_min=-2**3, quant_max=2**3 - 1))\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 4), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T07:23:55.015636Z",
     "end_time": "2023-07-10T07:24:44.478266Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model quantized to 2-bit with per channel quantization scheme (custom Observer implementation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantized model (per tensor scheme) took 33.21948s.\n",
      "Quantized model test loss: 2.3041255474090576\n",
      "Quantized model test accuracy: 0.1002769023180008\n",
      "(Theoretical) memory consumption of model: 0.07Mb\n"
     ]
    }
   ],
   "source": [
    "from src.ptq.activation_observer import PerChannelObserver, SimpleObserver\n",
    "\n",
    "timer = Timer(\"2-bit quantized model (per channel scheme)\")\n",
    "num_calibration_batches = 20\n",
    "\n",
    "model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.qconfig.QConfig(SimpleObserver.with_args(dtype=torch.quint8,\n",
    "                                                                               qscheme=torch.per_tensor_affine, quant_min=0, quant_max=2**2-1),\n",
    "                                                      PerChannelObserver.with_args(dtype=torch.qint8,\n",
    "                                                                                   qscheme=torch.per_channel_symmetric, quant_min=-2, quant_max=1))\n",
    "model.fuse_model()\n",
    "model = torch.ao.quantization.prepare(model, inplace=True)\n",
    "_, _ = evaluate(model, criterion, train_loader, num_batches=num_calibration_batches)\n",
    "model_q = torch.ao.quantization.convert(model)\n",
    "\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print(f\"Quantized model test loss: {val_loss.item()}\")\n",
    "print(f\"Quantized model test accuracy: {val_accuracy.item()}\")\n",
    "print(f\"(Theoretical) memory consumption of model: {round(memory_consumption(model, 2), 3)}Mb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T07:23:05.219329Z",
     "end_time": "2023-07-10T07:23:54.385893Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QAT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qat_model = ResNet20(configuration=(3, 2, 2), num_classes=10, quantize=True)\n",
    "qat_model.load_state_dict(torch.load(\"checkpoints/resnet20_final.pth\"))\n",
    "qat_model.eval()\n",
    "qat_model.fuse_model()\n",
    "qat_model.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr=1e-5)\n",
    "qat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "\n",
    "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\n",
    "qat_model.cuda()\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T08:18:06.913544Z",
     "end_time": "2023-07-10T08:18:07.223091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def qat_train_epoch(model, criterion, optimizer, loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    losses = torch.zeros((1,), device=device)\n",
    "    for batch_id, (input, target) in enumerate(loader):\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses += loss.detach()\n",
    "\n",
    "    return losses / len(loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T08:18:07.305281Z",
     "end_time": "2023-07-10T08:18:07.366016Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1653,\ttest accuracy: 0.3895\n",
      "test loss: 0.1604,\ttest accuracy: 0.3898\n",
      "test loss: 0.1558,\ttest accuracy: 0.3896\n",
      "test loss: 0.153,\ttest accuracy: 0.3901\n",
      "test loss: 0.1495,\ttest accuracy: 0.39\n",
      "test loss: 0.1469,\ttest accuracy: 0.3902\n",
      "test loss: 0.1459,\ttest accuracy: 0.3899\n",
      "test loss: 0.1434,\ttest accuracy: 0.3903\n",
      "test loss: 0.142,\ttest accuracy: 0.3903\n",
      "test loss: 0.1408,\ttest accuracy: 0.3901\n",
      "QAT model took 32.70717s.\n",
      "\n",
      "QAT model test loss: 0.31536567211151123\n",
      "QAT model test accuracy: 0.909414529800415\n"
     ]
    }
   ],
   "source": [
    "timer = Timer(\"QAT model\")\n",
    "num_eval_batches = 32\n",
    "qat_train_epochs = 10\n",
    "\n",
    "for epoch in range(qat_train_epochs):\n",
    "    qat_train_epoch(qat_model, criterion, optimizer, train_loader, device=\"cuda\")\n",
    "    if epoch > 3:\n",
    "        qat_model.apply(torch.ao.quantization.disable_observer)\n",
    "    if epoch > 2:\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    model_q = torch.ao.quantization.convert(qat_model.cpu(), inplace=False)\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader, num_batches=num_eval_batches)\n",
    "    print(f\"test loss: {round(val_loss.item(),  4)},\\ttest accuracy: {round(val_accuracy.item(),  4)}\")\n",
    "\n",
    "model_q = torch.ao.quantization.convert(qat_model.cpu(), inplace=False)\n",
    "with timer:\n",
    "    val_loss, val_accuracy = evaluate(model_q, criterion, test_loader)\n",
    "\n",
    "print()\n",
    "print(f\"QAT model test loss: {val_loss.item()}\")\n",
    "print(f\"QAT model test accuracy: {val_accuracy.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T08:18:09.404632Z",
     "end_time": "2023-07-10T08:23:33.718191Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we additionally quantize QAT model using custom PTQ (to 4 and 2 bits)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
